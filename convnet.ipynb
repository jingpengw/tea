{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# julia implementation of convolutional network\n",
    "\n",
    "## prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prepare_data (generic function with 2 methods)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read and prepare the training and testing data\n",
    "using HDF5\n",
    "function prepare_data()\n",
    "    # read data\n",
    "    trn_fnm = \"data/train.hdf5\"\n",
    "    tst_fnm = \"data/test.hdf5\"\n",
    "    trn_d = h5read(trn_fnm, \"/data\")\n",
    "    trn_l = h5read(trn_fnm, \"/label\")\n",
    "\n",
    "    tst_d = h5read(tst_fnm, \"/data\")\n",
    "    tst_l = h5read(tst_fnm, \"/label\")\n",
    "\n",
    "    # reshape the data\n",
    "    trn_d = reshape(trn_d, (784, 60000))\n",
    "    trn_l = reshape(trn_l, 60000)\n",
    "    tst_d = reshape(tst_d, (784, 10000))\n",
    "    tst_l = reshape(tst_l, 10000)\n",
    "\n",
    "    # create tuples of data and label\n",
    "    # transform the label number to a vector matching network output\n",
    "    trn = Array((Vector, Vector), 0)\n",
    "    for k in 1:length( trn_l )\n",
    "        y = zeros(10)\n",
    "        y[ trn_l[k]+1 ] = 1.0\n",
    "        push!(trn, (trn_d[:,k], y))\n",
    "    end\n",
    "\n",
    "    tst = Array((Vector, Vector), 0)\n",
    "    for k in 1:length(tst_l)\n",
    "        y = zeros(10)\n",
    "        y[ tst_l[k]+1 ] = 1.0\n",
    "        push!(tst, (tst_d[:,k], y))\n",
    "    end\n",
    "    \n",
    "    return (trn, tst)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (generic function with 1 method)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type Network\n",
    "    num_layers::Uint32\n",
    "    sizes::Vector{Uint32}\n",
    "    biases::Vector{Array}\n",
    "    weights::Vector{Array}\n",
    "    \n",
    "    # inner construction function\n",
    "    Network()=new(0,{},{},{})\n",
    "    \n",
    "    function Network(sizes::Vector)\n",
    "        num_layers = length( sizes )\n",
    "        biases = {[0]}\n",
    "        weights = {[0]}\n",
    "        for li in 2:num_layers\n",
    "            push!(biases,  randn(sizes[li]) )\n",
    "            push!(weights, randn(sizes[li], sizes[li-1]) ) \n",
    "        end\n",
    "        new(num_layers, sizes, biases, weights)\n",
    "    end\n",
    "end\n",
    "\n",
    "# return the vector of partial derivatives \\partial C_x/ \\partial a for the output activations\n",
    "function cost_derivative(output_activations::Vector, y::Vector)\n",
    "    return output_activations - y\n",
    "end\n",
    "\n",
    "# sigmoid function\n",
    "function sigmoid( z )\n",
    "    return 1.0./(1.0+exp(-z))\n",
    "end\n",
    "\n",
    "function sigmoid_prime(z)\n",
    "    return sigmoid(z).*(1-sigmoid(z))\n",
    "end\n",
    "\n",
    "# define the functions of Network\n",
    "function feedforward(net::Network, a::Array)\n",
    "    # the input a is the first layer\n",
    "    @inbounds @simd for li in 2:net.num_layers\n",
    "        a = sigmoid( net.weights[li]*a + net.biases[li] )\n",
    "    end\n",
    "    return a\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "# backpropagation\n",
    "# Input : network \n",
    "# output: a tuple (nabla_b, nabla_w) representing the gradient for the cost function C_x.\n",
    "function backprop(net::Network, x::Vector, y::Vector)\n",
    "    \n",
    "    # feedforward\n",
    "    activation = x\n",
    "    # list to store all the activations, layer by layer\n",
    "    activations = {x}\n",
    "    # list to store all the z vectors, layer by layer\n",
    "    zs = {x}\n",
    "    @inbounds @simd for li in 2:net.num_layers\n",
    "        z = net.weights[li]*activation + net.biases[li]\n",
    "        push!(zs,z)\n",
    "        activation = sigmoid(z)\n",
    "        push!(activations, activation)\n",
    "    end\n",
    "    \n",
    "    # backward pass\n",
    "    nabla_b = {[0]}\n",
    "    nabla_w = {[0]}\n",
    "    for li in 2:net.num_layers\n",
    "        push!(nabla_b, zeros(net.biases[li]))\n",
    "        push!(nabla_w, zeros(net.weights[li]))\n",
    "    end\n",
    "    delta = cost_derivative(activations[end], y) .* sigmoid_prime(zs[end])\n",
    "    nabla_b[end] = delta\n",
    "    nabla_w[end] = delta * transpose(activations[end-1])\n",
    "    @inbounds @simd for li in net.num_layers-1:-1:2\n",
    "#         println(\"new left: $(size(net.weights[li+1]))\")\n",
    "#         println(\"new right: $(size(delta))\")\n",
    "#         println(\"spv: $(size(spv))\")\n",
    "#         println(\"z: $(size(z))\")\n",
    "#         println(\"li: $(li)\")\n",
    "        delta = (transpose(net.weights[li+1]) * delta) .* sigmoid_prime(zs[li])\n",
    "        nabla_b[li] = delta\n",
    "        nabla_w[li] = delta * transpose(activations[li-1])\n",
    "    end\n",
    "    return (nabla_b, nabla_w)\n",
    "end\n",
    "\n",
    "# update the weights and biases by applying gradient descent using backpropagation to a single mini batch.\n",
    "# mini_batch: a list of tuples \"(x,y)\"\n",
    "# eta: the learning rate\n",
    "function update_mini_batch!(net::Network, mini_batch::Array{(Vector, Vector)}, eta::Float32 )\n",
    "    # initialization\n",
    "    nabla_b = {[0]}\n",
    "    nabla_w = {[0]}\n",
    "    for li in 2:net.num_layers\n",
    "        push!(nabla_b, zeros(net.biases[li]))\n",
    "        push!(nabla_w, zeros(net.weights[li]))\n",
    "    end\n",
    "    for xy in mini_batch\n",
    "        x,y = xy\n",
    "        delta_nabla_b, delta_nabla_w = backprop(net, x, y)\n",
    "        @inbounds @simd for li in 2:net.num_layers\n",
    "            nabla_b[li] = nabla_b[li] + delta_nabla_b[li]\n",
    "            nabla_w[li] = nabla_w[li] + delta_nabla_w[li]\n",
    "        end\n",
    "    end\n",
    "    # update the weights and biases using minibatch results\n",
    "    @inbounds @simd for li in 2:net.num_layers\n",
    "        net.biases[li]  = net.biases[li]  - eta*(nabla_b[li]./length(mini_batch))\n",
    "        net.weights[li] = net.weights[li] - eta*(nabla_w[li]./length(mini_batch))\n",
    "    end\n",
    "end\n",
    "\n",
    "# evaluate the results\n",
    "function evaluate(net::Network, tst)\n",
    "    # correct number\n",
    "    cn = 0\n",
    "    for xy in tst\n",
    "        x,y = xy\n",
    "        yt = feedforward(net, x)\n",
    "        if indmax(yt)==indmax(y)\n",
    "            cn = cn + 1\n",
    "        end\n",
    "    end\n",
    "    return cn./length(tst)\n",
    "end\n",
    "\n",
    "# \n",
    "function SGD( net::Network, trn::Vector{(Vector, Vector)}, epoches::Uint32, \n",
    "    mini_batch_size::Uint32, eta::Float32, tst::Vector{(Vector, Vector)} )\n",
    "    println(\"starting stochastic gradient descent training...\")\n",
    "    # number of samples\n",
    "    n_tst = length( tst )\n",
    "    n_trn = length( trn )\n",
    "    \n",
    "    for j in 1:epoches\n",
    "        print(\"Epoch $(j)...\")\n",
    "        # shuffle the training data\n",
    "        shuffle!(trn)\n",
    "        # minibatch\n",
    "        for k in 1:mini_batch_size:n_trn\n",
    "            mini_batch = trn[k:k+mini_batch_size-1]\n",
    "            update_mini_batch!(net, mini_batch, eta)\n",
    "        end\n",
    "        \n",
    "        # test\n",
    "        cls = evaluate(net, tst)\n",
    "        println(\"cls: $(cls)\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: redefining constant sizes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting stochastic gradient descent training...\n",
      "Epoch 1...cls: 0.9134\n",
      "Epoch 2...cls: 0.9259\n",
      "Epoch 3..."
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "interrupt\nwhile loading In[63], in expression starting on line 4",
     "output_type": "error",
     "traceback": [
      "interrupt\nwhile loading In[63], in expression starting on line 4",
      "",
      " in gemv! at linalg/blas.jl:265",
      " in gemv! at linalg/matmul.jl:212",
      " in * at linalg/matmul.jl:68",
      " in backprop at In[62]:59",
      " in update_mini_batch! at In[62]:101",
      " in SGD at In[62]:147"
     ]
    }
   ],
   "source": [
    "trn, tst = prepare_data()\n",
    "const sizes = [784, 30, 10]\n",
    "net = Network( sizes )\n",
    "SGD(net, trn, uint32(30), uint32(10), float32(3.0), tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1003"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(net, tst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.7",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
